---
date: '1'
title: 'SCALe: Supervised Contrastive approach for Active Learning'
cover: './scale4.png'
github: 'https://github.com/kaushalrai7797/SCALe-Supervised-Contrastive-approach-for-Active-Learning'
external: 'https://github.com/kaushalrai7797/SCALe-Supervised-Contrastive-approach-for-Active-Learning/blob/main/SCALe_Supervised_Contrastive_approach_for_Active_Learning.pdf'
tech:
- Python
- NLP
- SupCL
- MLM
---

- Proposed a novel loss function (supervised contrastive loss) for active learning that generates multiple positives using dropout maskings.
- Concluded that using MLM to predict missing tokens doesn't solve the objective of predicting dicriminative sentence embeddings for text classification.
- Showed that using contrastive learning is better than MLM for sentence representation in low-data regime. Embeddings generated by contrastive learning are discriminative.
- The proposed loss function lead to an improvement of 10% on the Trec-6 dataset and up to 4% on SST-2 dataset on 15% acquired data, thereby beating the current state-of-the-art method.
