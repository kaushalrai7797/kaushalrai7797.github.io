{"expireTime":9007200912219983000,"key":"transformer-remark-markdown-html-2edcbedabccd222b1098fa8f47f906f2-gatsby-remark-external-linksgatsby-remark-imagesgatsby-remark-code-titlesgatsby-remark-prismjs-","val":"<ul>\n<li>Proposed a novel loss function (supervised contrastive loss) for active learning that generates multiple positives using dropout maskings.</li>\n<li>Concluded that using MLM to predict missing tokens doesn't solve the objective of predicting dicriminative sentence embeddings for text classification.</li>\n<li>Showed that using contrastive learning is better than MLM for sentence representation in low-data regime. Embeddings generated by contrastive learning are discriminative.</li>\n<li>The proposed loss function lead to an improvement of 10% on the Trec-6 dataset and up to 4% on SST-2 dataset on 15% acquired data, thereby beating the current state-of-the-art method.</li>\n</ul>"}